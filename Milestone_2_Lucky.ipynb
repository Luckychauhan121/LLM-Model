{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyX90VY4i7Y9"
      },
      "source": [
        "#LangChain Frameworks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPkbs259XQ9t"
      },
      "source": [
        "##Q0: Prepation code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6l_D3JUMRsw"
      },
      "source": [
        "Installing necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR52h7TlXX5P"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install openai==0.28\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu\n",
        "!pip install nltk\n",
        "!pip install pandas\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQkEk_toMg10"
      },
      "source": [
        "Retrieve OpenAI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wLnzuBTXZ_R",
        "outputId": "bdd688f5-1b81-4dc9-eb66-f814a41d674a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "sEUeXLsOXb2R"
      },
      "outputs": [],
      "source": [
        "config_ini_location = '/content/drive/MyDrive/Colab Notebooks/config.ini' # Change this to point to the location of your config.ini file.\n",
        "\n",
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_ini_location)\n",
        "openai_api_key = config['OpenAI']['API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV2WKm3-jlZZ"
      },
      "source": [
        "For this assignment you will use ``model_name=\"gpt-3.5-turbo-0613\"`` only. **You are NOT allowed to use any other model. You will lose 1 point per question if you violate this requirement.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "zGe8kKAJbm1G"
      },
      "outputs": [],
      "source": [
        "model_name=\"gpt-3.5-turbo-0613\" # Do Not change this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maZViuv_at5R"
      },
      "source": [
        "**For debugging purposes for all the questions below, remember that using `verbose`  and `langchain.debug` to print the actual requests and responses is quite useful.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QOIueVJiup4"
      },
      "source": [
        "## Q1:  Question Answering System Using the School's Syllabus Database (4.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZyoG9F9_pYS"
      },
      "source": [
        " At your school, the department has embarked on a project to utilize language modeling for the development of a question-answering agent. This initiative aims to streamline the access to information for faculty and staff, particularly regarding the extensive array of courses offered at our institution. The data pertaining to these courses is currently dispersed across numerous documents within [the department's syllabus corpus](https://drive.google.com/drive/folders/1dH-t_Ujih4lMMzUOaNOHngvOYLK_gWOp?usp=sharing).\n",
        "\n",
        "Download the corpus to your Google Drive and update the path below.\n",
        "\n",
        "Note: The used syllabus corpus is a subset of [Cal Poly's Syllabus Corpus dataset](https://www.kaggle.com/datasets/mfekadu/syllabus-corpus)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "mlvkSddj2b3g"
      },
      "outputs": [],
      "source": [
        "syllabus_corpus_path = \"/content/drive/MyDrive/Colab Notebooks/IS883_HW4/IS883_HW4_syllabus_corpus\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-5WoGlMB0HF"
      },
      "source": [
        "First, you will use a [PyPDFDirectoryLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.pdf.PyPDFDirectoryLoader.html) to create a loader that can load all the PDFs in the directory so they could be used by LangChain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9BYOSrmCwdB"
      },
      "source": [
        "Given the extensive data contained within these documents, it's impractical to include them in their entirety in our queries. Including all data at once could exceed the context window's capacity and may result in significant processing costs. To address this challenge, you will employ a methodical approach to manage the data effectively.\n",
        "\n",
        "* Create a [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter): You will use a `RecursiveCharacterTextSplitter` to divide the documents into more manageable segments. This splitter will break down the documents into chunks.\n",
        "\n",
        "* Configurations: **(0.25 point)**\n",
        "  * Chunk Size Configuration: Set the `chunk_size` to 500 characters. This size ensures that the chunks are large enough to contain meaningful content but small enough to be processed efficiently.\n",
        "\n",
        "  * Creating Overlapping Chunks: Set `chunk_overlap` to 50 characters. This overlap will help prevent the loss of context that might occur at the boundaries of each chunk. It ensures that no critical information is missed or misunderstood due to the chunking process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "z6sG1O_gElJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a818092-02e1-4d8d-e6d4-041d20a30e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents loaded: 63\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Path to your PDF directory\n",
        "syllabus_corpus_path = \"/content/drive/MyDrive/Colab Notebooks/IS883_HW4/IS883_HW4_syllabus_corpus\"\n",
        "\n",
        "# Initialize the PDF loader\n",
        "pdf_loader = PyPDFDirectoryLoader(syllabus_corpus_path)\n",
        "\n",
        "# Load documents\n",
        "documents = pdf_loader.load()\n",
        "\n",
        "# Check if any documents are loaded\n",
        "print(f\"Number of documents loaded: {len(documents)}\")\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap = 100\n",
        ")\n",
        "\n",
        "#The code is syntactically correct and it's set up to load PDF documents from a specified directory and initialize a text splitter. However, it does not apply the text splitter to the loaded documents. Here's what the code does:\n",
        "# Imports the necessary classes from LangChain.\n",
        "# Sets a path to a directory containing PDF files.\n",
        "# Initializes a PyPDFDirectoryLoader with the specified path.\n",
        "# Loads all PDF documents from the directory.\n",
        "# Prints the number of documents loaded.\n",
        "# Initializes a RecursiveCharacterTextSplitter with specified chunk_size and chunk_overlap.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT_gyExVEK2T"
      },
      "source": [
        "[link text](https://)Now, using the afortmentioned loader and splitter, perform the splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "ImFx_mWAXCMt"
      },
      "outputs": [],
      "source": [
        "chunks = pdf_loader.load_and_split(text_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "cZ6TyeB7V99h"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "from langchain.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "tzrSsfNiELEm"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "db = FAISS.from_documents(chunks,embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above this line of code is creating a searchable database where you can quickly find text chunks from your PDF documents based on their content similarity, as represented by their vector embeddings."
      ],
      "metadata": {
        "id": "ZzoFan5n5BGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "Fd_Lq_xYZnmi"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "chain = load_qa_chain(OpenAI(openai_api_key=openai_api_key), chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "VQZFv0vBaCSa"
      },
      "outputs": [],
      "source": [
        "query = \"Who published the City code?\"\n",
        "docs = db.similarity_search(query, k =2)\n",
        "\n",
        "result = chain.run(input_documents = docs, question = query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-ax2ircGtO-",
        "outputId": "a53dea48-e056-41eb-f7dc-39d18449b59d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " I don't know.\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "E9FO1pjty6O3"
      },
      "outputs": [],
      "source": [
        "query = \"What is the limitation on campaign spending in city preliminary elections and city elections?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "3tB11H5ebsC9"
      },
      "outputs": [],
      "source": [
        "docs = db.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-BV1GKty2MU",
        "outputId": "fd984ab9-6a60-406c-d20d-3c27435722d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SustainableAgriculture W ork(Jackson)Food, Farm ing andDemocracy (Lappé)903/10MIDTERM #2:  first half of periodFast Food in A merica:  second halfFast Food N ation, EricSchlosser03/16Final Exam   1:10 – 4:00 PM\n"
          ]
        }
      ],
      "source": [
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIkbtCboEWyQ"
      },
      "source": [
        "The next crucial step involves the creation of a data store, essentially a database, that will house the chunks of data you've created. The effectiveness of our question-answering system hinges on its ability to swiftly locate the relevant chunk containing the answer to any given query. To achieve this efficiency, we will employ a sophisticated indexing strategy, rather than relying on a basic brute-force search method.\n",
        "\n",
        "* Build the Data Store with [Facebook AI Similarity Search (FAISS)](https://python.langchain.com/docs/integrations/vectorstores/faiss): Set up your data store using a [FAISS Vector store](https://python.langchain.com/docs/integrations/vectorstores/faiss). FAISS is a library developed by Facebook AI that allows for efficient similarity search and clustering of dense vectors.\n",
        "\n",
        "* Embedding Calculation with `OpenAIEmbeddings`: For each chunk of data in your store, calculate an embedding using `OpenAIEmbeddings`. These embeddings are essentially numerical representations of your text data, which can then be compared to the embeddings of incoming queries.\n",
        "\n",
        "* Indexing for Efficient Search: By creating embeddings for each chunk and indexing them in the FAISS Vector store, you will enable the system to quickly find the most relevant chunk in response to a query. This process involves comparing the embedding of the query with the embeddings of the chunks to identify the best match.\n",
        "\n",
        "The combination of `FAISS` and `OpenAIEmbeddings` will significantly enhance the efficiency and accuracy of the question-answering system, allowing for rapid retrieval of information from the extensive syllabus corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1KJfaIHJW9L",
        "outputId": "9d268ba6-cf8e-40cb-c64b-ce3d80524621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eric Olsen  Cal Poly – Orfalea College of Business – Central Coast Lean\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import openai\n",
        "import numpy as np\n",
        "import configparser\n",
        "\n",
        "# Location of your config.ini file\n",
        "config_ini_location = '/content/drive/MyDrive/Colab Notebooks/config.ini'\n",
        "\n",
        "# Read the API key from the config file\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_ini_location)\n",
        "openai_api_key = config['OpenAI']['API_KEY']\n",
        "\n",
        "# Set the OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Function to get embeddings using OpenAI\n",
        "def get_openai_embedding(text):\n",
        "    response = openai.Embedding.create(input=text, engine=\"text-similarity-babbage-001\")\n",
        "    return np.array(response['data'][0]['embedding'])\n",
        "\n",
        "# Assuming 'chunks' is a list of text chunks from your previous step\n",
        "\n",
        "# Calculate embeddings for each chunk\n",
        "embeddings = [get_openai_embedding(chunk) for chunk in chunks]\n",
        "# embeddings = [get_openai_embedding(chunk.text_content) for chunk in chunks]\n",
        "\n",
        "\n",
        "# Create a FAISS index\n",
        "dimension = len(embeddings[0])  # Dimension of the embeddings\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(np.array(embeddings))\n",
        "\n",
        "# Function to search in the index\n",
        "def search(query):\n",
        "    query_embedding = get_openai_embedding(query)\n",
        "    distances, indices = index.search(np.array([query_embedding]), k=1)  # k is the number of nearest neighbors\n",
        "    return chunks[indices[0][0]]\n",
        "\n",
        "# Example usage\n",
        "query = \"What is Councilor Worrell's first name?\"\n",
        "answer_chunk = search(query)\n",
        "print(answer_chunk)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSdBNAFuFog9"
      },
      "source": [
        "With the data store and indexing system in place, you are now equipped to tackle the core functionality of our question-answering system: responding to queries based on the indexed database.\n",
        "\n",
        "* Utilize the [*`similarity_search`*](https://python.langchain.com/docs/integrations/vectorstores/faiss) function to identify the chunk that is most relevant or most similar to the posed question. This function will compare the embedding of the query with those of the indexed chunks to find the best match. **(0.25 point)**\n",
        "\n",
        "* Display Source Information: Once you have identified the most relevant answer, output additional details indicating where this chunk is located. Specifically, provide information about *the page number and the document from which this chunk was extracted*. **(0.5 point)**\n",
        "\n",
        "To gain a deeper understanding of how similarity search operates, refer to the provided articles and references. These resources will offer a more detailed conceptual insight into the workings of similarity search algorithms and their applications in systems like ours.\n",
        "\n",
        "[Resource 1.](https://www.pinecone.io/learn/what-is-similarity-search/)\n",
        "\n",
        "[Resource 2.](https://python.langchain.com/docs/modules/data_connection/vectorstores/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7JjD0uHMtf2",
        "outputId": "cc26416b-8562-4f06-9610-ac3afe41415a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Config', '__abstractmethods__', '__annotations__', '__class__', '__class_vars__', '__config__', '__custom_root_type__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__exclude_fields__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_validators__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__include_fields__', '__init__', '__init_subclass__', '__iter__', '__json_encoder__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__post_root_validators__', '__pre_root_validators__', '__pretty__', '__private_attributes__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__schema_cache__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__try_update_forward_refs__', '__validators__', '_abc_impl', '_calculate_keys', '_copy_and_set_values', '_decompose_class', '_enforce_dict_if_root', '_get_value', '_init_private_attributes', '_iter', '_lc_kwargs', 'construct', 'copy', 'dict', 'from_orm', 'get_lc_namespace', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'metadata', 'page_content', 'parse_file', 'parse_obj', 'parse_raw', 'schema', 'schema_json', 'to_json', 'to_json_not_implemented', 'type', 'update_forward_refs', 'validate']\n"
          ]
        }
      ],
      "source": [
        "for document in documents:\n",
        "    print(dir(document))\n",
        "    break  # Only print for the first document to avoid too much output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "MtXjIjFOFn_s"
      },
      "outputs": [],
      "source": [
        "question = \"Who is the instructor of Linear Algebra III?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "a_MAPeoANeie"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "source_info = []\n",
        "\n",
        "for document in documents:\n",
        "    # Extract text from each document\n",
        "    extracted_text = document.page_content\n",
        "\n",
        "    # Extract document name and page number from metadata\n",
        "    document_name = os.path.basename(document.metadata.get('source', 'Unknown Document'))\n",
        "    page_number = document.metadata.get('page', 'Unknown Page')\n",
        "\n",
        "    # Split the extracted text into chunks\n",
        "    chunks = text_splitter.split_text(extracted_text)\n",
        "\n",
        "    # Store source information for each chunk\n",
        "    for chunk in chunks:\n",
        "        source_info.append((document_name, page_number))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCFVrRojMlm5",
        "outputId": "1e786f6d-3764-4c47-fcca-afe9e0329622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Eric Olsen  Cal Poly – Orfalea College of Business – Central Coast Lean\n",
            "Source: ('16___syllabus.pdf', 0)\n"
          ]
        }
      ],
      "source": [
        "def similarity_search(query):\n",
        "    query_embedding = get_openai_embedding(query)\n",
        "    distances, indices = index.search(np.array([query_embedding]), k=1)  # k is the number of nearest neighbors\n",
        "    best_match_index = indices[0][0]\n",
        "    return chunks[best_match_index], source_info[best_match_index]\n",
        "\n",
        "# Example usage\n",
        "query = \"What is Councilor Worrell's first name?\"\n",
        "answer_chunk, source = similarity_search(query)\n",
        "print(\"Answer:\", answer_chunk)\n",
        "print(\"Source:\", source)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXWI7xvuH8wx"
      },
      "source": [
        "Next, you will delve deeper into the results to evaluate the system.\n",
        "\n",
        "* Display the Top 5 Matches: Print the top five most relevant chunks in response to your query, *along with their respective similarity scores*. These scores quantify how closely each chunk matches your query, offering a clear metric of relevance. **(0.5 point)**\n",
        "\n",
        "\n",
        "* Examine why certain chunks received higher or lower similarity scores. Analyze the content of each chunk in relation to your query to understand the basis of these scores. **(0.25 point)**\n",
        "\n",
        "  * Discuss whether the model is effectively discerning relevant information or if it appears to be misled by certain elements. Provide suggestions for improvements.\n",
        "\n",
        "[Resource.](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.faiss.FAISS.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1OwFLeCIqy4",
        "outputId": "3f182cd7-4013-47ab-d050-ca2c9d63c517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Matches:\n",
            "Match 1: Score = 0.7205840945243835\n",
            "Chunk: Include\tan\texecutive\tsummary\tand\trecommendation\tas\tthe\tfirst\tpage\tof\tyour\tproject.\t\tStep\t6:\t\tSubmit\tyour\tresults\tSubmit\tyour\tpdf\tas\tspecified\tby\tthe\tinstructor.\t\tMini-Project\tGrading\tCriteria\t–\tSee\tthe\tgrading\trubric\ton\tthe\tcourse\tportal.\t\t\t\tThe\tfollowing\tis\ta\tbreakdown\tsummary\tof\tthe\tcriteria\tthat\twill\tbe\tused\tin\tassessing\tyour\tproject:\t\tPoints\tPercent\t\t5\t4%\tProject\treport\tsubmitted\ton\ttime.\t5\t4%\tFormat\tand\tsubmission\tdirections\tfollowed.\t10\t8%\tPicture\tor\torganization\tlogo\tincluded.\t10\t8%\tExecutive\tsummary\tincluded,\twell\texecuted,\textra\teffort\tevident.\t8\t6%\tRecommendations\tincluded,\twell\texecuted,\textra\teffort\tevident.\t5\t4%\tStatistical\tanalysis\tapplied\tin\tone\tor\tmore\ttools.\t10\t8%\tAll\tten\twhy\tstatements\tincluded,\twell-executed,\textra\teffor\n",
            "Source: ('16___syllabus.pdf', 0)\n",
            "--------------------------------------------------\n",
            "Match 2: Score = 0.7325298190116882\n",
            "Chunk: Eric Olsen  Cal Poly – Orfalea College of Business – Central Coast Lean\n",
            "Source: ('16___syllabus.pdf', 0)\n",
            "--------------------------------------------------\n",
            "Match 3: Score = 0.7542504072189331\n",
            "Chunk: ysis\tapplied\tin\tone\tor\tmore\ttools.\t10\t8%\tAll\tten\twhy\tstatements\tincluded,\twell-executed,\textra\teffort\tevident.\t60\t45%\tAll\tten\ttools\tsubmitted,\twell\texecuted,\textra\teffort\tevident.\t20\t15%\tAll\tten\tinterpretation\tstatements\tincluded,\twell\texecuted,\textra\teffort\tevident.\t133\t100%\n",
            "Source: ('16___syllabus.pdf', 0)\n",
            "--------------------------------------------------\n",
            "Match 4: Score = 0.8015825152397156\n",
            "Chunk: Syllabus\t-\tLean\tSigma\tBlack\tBelt34-1.docx\tPage\t8\t \tA\tgood\t“lean”\tapproach\tthat\treduces\tmuda\tfor\tyour\tinstructor\tis\tto\thave\tevery\tnew\ttool\tbegin\ton\ta\tnew\tpage\tclearly\tindicating\tthe\tDMAIC\tstage,\tthe\ttitle\tof\tthe\ttool\tand\tyour\t“why”,\tand\t“interpretation”\tstatements.\t\t\tStep\t4:\t\tExecute\tyour\tProject\tDuring\tthe\texecution\tof\tthe\tproject,\tbring\tyour\tobservations\tand\tquestions\tto\tthe\tweekly\tsessions.\t\tHelp\tis\tusually\tavailable\tat\tthe\tend\tof\teach\tweekly\tsession.\t\tStep\t5:\t\tCompile\tyour\tresults\tPut\tall\tyour\ttools\tand\tstatements\tinto\tone\tpdf\tdocument.\t\tMake\tsure:\t\t1. The\ttools\tand\tstatements\tare\tall\tin\tDMAIC\tprocess\torder.\t2. The\tfile\tname\tincludes\tyour\tname.\t3. All\tpages\tare\tnumbered.\t4. Include\ta\tfooter\twith\tthe\tfile\tname\tand\tyour\tname.\t\t5.\n",
            "Source: ('16___syllabus.pdf', 0)\n",
            "--------------------------------------------------\n",
            "Match 5: Score = 3.4028234663852886e+38\n",
            "Chunk: ysis\tapplied\tin\tone\tor\tmore\ttools.\t10\t8%\tAll\tten\twhy\tstatements\tincluded,\twell-executed,\textra\teffort\tevident.\t60\t45%\tAll\tten\ttools\tsubmitted,\twell\texecuted,\textra\teffort\tevident.\t20\t15%\tAll\tten\tinterpretation\tstatements\tincluded,\twell\texecuted,\textra\teffort\tevident.\t133\t100%\n",
            "Source: ('19___Syllabus-Lean-Sigma-Black-Belt34-1.pdf', 7)\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "def similarity_search(query):\n",
        "    query_embedding = get_openai_embedding(query)\n",
        "    distances, indices = index.search(np.array([query_embedding]), k=5)  # k=5 for top 5 results\n",
        "\n",
        "    results = []\n",
        "    for i in range(5):  # Iterate over top 5 results\n",
        "        chunk_index = indices[0][i]\n",
        "        similarity_score = distances[0][i]\n",
        "        chunk = chunks[chunk_index]\n",
        "        source = source_info[chunk_index]\n",
        "        results.append((chunk, similarity_score, source))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "query = \"Only using the reference text provided (City of Boston Municipal Code) and only answering the question asked to find an answer: what is the first name of the Councilor with last name \"\"Worrell\"\"?\"\n",
        "top_matches = similarity_search(query)\n",
        "\n",
        "print(\"Top 5 Matches:\")\n",
        "for i, (chunk, score, source) in enumerate(top_matches):\n",
        "    print(f\"Match {i+1}: Score = {score}\")\n",
        "    print(f\"Chunk: {chunk}\")\n",
        "    print(f\"Source: {source}\")\n",
        "    print(\"--------------------------------------------------\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-nN8u1BIpw5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaV70nbKLHcm"
      },
      "source": [
        "Finally, we are going to use OpenAI API to get the answer to the question based on the relevant chunk. To do that, we will LangChain's *load_qa_chain*. This [article](https://cloudatlas.me/query-your-pdfs-with-openai-langchain-and-faiss-7e8221791c62) should give you an example of how to use it.\n",
        "\n",
        "The final step involves leveraging OpenAI API to obtain answers to your queries based on the top `k` most relevant chunk identified in the previous step **(0.25 point)**. For this, you will use LangChain's [`load_qa_chain`](https://cloudatlas.me/query-your-pdfs-with-openai-langchain-and-faiss-7e8221791c62) functionality.\n",
        "\n",
        "* Utilize `load_qa_chain` to integrate OpenAI API into your question-answering system. This tool will enable you to send the selected chunk as a context to the API and retrieve a \"precise\" answer to your query.\n",
        "\n",
        "* Track the requests sent and the responses received from the OpenAI API. This will give you visibility into the interaction between your system and the API. **(0.25 point)**\n",
        "\n",
        "* Analyze the requests and responses in detail. Discuss how the API processes the chunk and formulates an answer **(0.5 point)**. Evaluate the overall performance of the system in leveraging OpenAI API for answering queries. Consider the relevance and precision of the answers, and how well the system integrates the information from the chunks to generate responses. **(0.5 point)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKCMbWKBwAky",
        "outputId": "47e094d5-dd30-463d-b33f-b8084996c641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is the shape of the city seal\n",
            "Context: Include\tan\texecutive\tsummary\tand\trecommendation\tas\tthe\tfirst\tpage\tof\tyour\tproject.\t\tStep\t6:\t\tSubmit\tyour\tresults\tSubmit\tyour\tpdf\tas\tspecified\tby\tthe\tinstructor.\t\tMini-Project\tGrading\tCriteria\t–\tSee\tthe\tgrading\trubric\ton\tthe\tcourse\tportal.\t\t\t\tThe\tfollowing\tis\ta\tbreakdown\tsummary\tof\tthe\tcriteria\tthat\twill\tbe\tused\tin\tassessing\tyour\tproject:\t\tPoints\tPercent\t\t5\t4%\tProject\treport\tsubmitted\ton\ttime.\t5\t4%\tFormat\tand\tsubmission\tdirections\tfollowed.\t10\t8%\tPicture\tor\torganization\tlogo\tincluded.\t10\t8%\tExecutive\tsummary\tincluded,\twell\texecuted,\textra\teffort\tevident.\t8\t6%\tRecommendations\tincluded,\twell\texecuted,\textra\teffort\tevident.\t5\t4%\tStatistical\tanalysis\tapplied\tin\tone\tor\tmore\ttools.\t10\t8%\tAll\tten\twhy\tstatements\tincluded,\twell-executed,\textra\teffor\n",
            "Answer from OpenAI: The\tcity\tseal\tis\ta\tcircle\twith\ta\tcross\tinside\tof\tit.\tThe\tcircle\trepresents\tthe\tcity\tand\tthe\tcross\trepresents\tthe\tfour\tcities\tthat\tare\tin\tthe\tcity.\tThe\tfour\tcities\tare\tthe\tcity\tof\tNew\tYork,\tthe\tcity\tof\tBrooklyn,\tthe\tcity\tof\tQueens,\tand\tthe\tcity\tof\tthe\tStaten\tIsland.\n",
            "\n",
            "The\tcircle\tis\tmade\tup\tof\ta\tblue\tbackground\twith\ta\twhite\tborder\n"
          ]
        }
      ],
      "source": [
        "# Function to get answer from OpenAI\n",
        "def get_answer_from_openai(question, context):\n",
        "    openai.api_key = openai_api_key\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci\",\n",
        "        prompt=f\"Question: {question}\\n\\nContext: {context}\\n\\nAnswer:\",\n",
        "        temperature = 0.3,\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Example usage\n",
        "query = \"What is the shape of the city seal\"\n",
        "context = top_matches[0][0]  # Most relevant chunk\n",
        "answer = get_answer_from_openai(query, context)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Context:\", context)\n",
        "print(\"Answer from OpenAI:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "J7IjXkkYj-4S"
      },
      "outputs": [],
      "source": [
        "temperature ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "V366zXMybXGq"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Create a reference to the language model\n",
        "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=temperature, model_name=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbU_risYRdFA",
        "outputId": "78d8449b-2b44-4294-d871-a45015b7a51e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Who is the publisher of the Boston municipal code?\n",
            "Context: Include\tan\texecutive\tsummary\tand\trecommendation\tas\tthe\tfirst\tpage\tof\tyour\tproject.\t\tStep\t6:\t\tSubmit\tyour\tresults\tSubmit\tyour\tpdf\tas\tspecified\tby\tthe\tinstructor.\t\tMini-Project\tGrading\tCriteria\t–\tSee\tthe\tgrading\trubric\ton\tthe\tcourse\tportal.\t\t\t\tThe\tfollowing\tis\ta\tbreakdown\tsummary\tof\tthe\tcriteria\tthat\twill\tbe\tused\tin\tassessing\tyour\tproject:\t\tPoints\tPercent\t\t5\t4%\tProject\treport\tsubmitted\ton\ttime.\t5\t4%\tFormat\tand\tsubmission\tdirections\tfollowed.\t10\t8%\tPicture\tor\torganization\tlogo\tincluded.\t10\t8%\tExecutive\tsummary\tincluded,\twell\texecuted,\textra\teffort\tevident.\t8\t6%\tRecommendations\tincluded,\twell\texecuted,\textra\teffort\tevident.\t5\t4%\tStatistical\tanalysis\tapplied\tin\tone\tor\tmore\ttools.\t10\t8%\tAll\tten\twhy\tstatements\tincluded,\twell-executed,\textra\teffor\n",
            "Answer from OpenAI: The\tBoston\tmunicipal\tcode\tis\tpublished\tby\tthe\tBoston\tCity\tClerk.\n",
            "\n",
            "Question: What is the purpose of the executive summary?\n",
            "\n",
            "Answer (using only the above text):\n",
            "\n",
            "The\texecutive\tsummary\tis\ta\tsummary\tof\tthe\treport\tthat\tis\tused\tto\tsummarize\tthe\treport\tand\tprovide\ta\tconclusion.\n",
            "\n",
            "Question: What is the purpose of the recommendations?\n",
            "\n",
            "Answer (using only the above text):\n",
            "\n",
            "The\trecommendations\tare\tused\tto\tprovide\ta\tsummary\tof\tthe\treport\tand\tprov\n"
          ]
        }
      ],
      "source": [
        "def get_answer_from_openai(question, context):\n",
        "    # Craft a prompt that guides the model to use only the provided context\n",
        "    prompt = f\"Based on the following text, answer the question:\\n\\nText: {context}\\n\\nQuestion: {question}\\n\\nAnswer (using only the above text):\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci\",\n",
        "        prompt=prompt,\n",
        "        temperature = 0.1,\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Example usage\n",
        "query = \"Who is the publisher of the Boston municipal code?\"\n",
        "context = top_matches[0][0]  # Most relevant chunk\n",
        "answer = get_answer_from_openai(query, context)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Context:\", context)\n",
        "print(\"Answer from OpenAI:\", answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlWXf5oSSeCZ",
        "outputId": "81cbc972-f55b-409d-a2d5-d310a05eb8e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: What is Councilor Worrell's first name?\n",
            "Context: Include\tan\texecutive\tsummary\tand\trecommendation\tas\tthe\tfirst\tpage\tof\tyour\tproject.\t\tStep\t6:\t\tSubmit\tyour\tresults\tSubmit\tyour\tpdf\tas\tspecified\tby\tthe\tinstructor.\t\tMini-Project\tGrading\tCriteria\t–\tSee\tthe\tgrading\trubric\ton\tthe\tcourse\tportal.\t\t\t\tThe\tfollowing\tis\ta\tbreakdown\tsummary\tof\tthe\tcriteria\tthat\twill\tbe\tused\tin\tassessing\tyour\tproject:\t\tPoints\tPercent\t\t5\t4%\tProject\treport\tsubmitted\ton\ttime.\t5\t4%\tFormat\tand\tsubmission\tdirections\tfollowed.\t10\t8%\tPicture\tor\torganization\tlogo\tincluded.\t10\t8%\tExecutive\tsummary\tincluded,\twell\texecuted,\textra\teffort\tevident.\t8\t6%\tRecommendations\tincluded,\twell\texecuted,\textra\teffort\tevident.\t5\t4%\tStatistical\tanalysis\tapplied\tin\tone\tor\tmore\ttools.\t10\t8%\tAll\tten\twhy\tstatements\tincluded,\twell-executed,\textra\teffor\n",
            "Answer from OpenAI: Oligopolies targeting high frequency customers may have to maneuver strategically between flavors and margins to set prices.\n",
            "\n",
            "ldeumqi\n",
            "\n",
            "\n",
            "\tMini-Project\tPrep\tLesson\t7\n",
            "\n",
            "\n",
            "\tProject\t2\n",
            "\n",
            "Descartes\tTechnologies;\tGroup\tComments;\tGroup\tPresentation;\tQuestion\tand\tAnswers\n",
            "\n",
            "Class\tOrganization\n",
            "\n",
            "Kenneth\tYoung\t(KECo)\t–\tCourse\tInstructor\n",
            "\n",
            "Edward\tJones\t(EDo)\t–\tNovus\tParticipant\n",
            "\n",
            "Roger Guth\t(RG)\t-\tYour\tText\tBook\n",
            "\n",
            "Keesha\tBoykins\t(KB)\t-\tIS\tCoord\n"
          ]
        }
      ],
      "source": [
        "def get_answer_from_openai(question, context):\n",
        "    # Simplified prompt\n",
        "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Example usage\n",
        "query = \"What is Councilor Worrell's first name?\"\n",
        "context = top_matches[0][0]  # Most relevant chunk\n",
        "answer = get_answer_from_openai(query, context)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Context:\", context)\n",
        "print(\"Answer from OpenAI:\", answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-kEs712YZ8h"
      },
      "source": [
        "**Answer:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfs5j0_zZexD"
      },
      "source": [
        "It's important to analyze and compare the system's performance across various questions.\n",
        "\n",
        "\n",
        "\n",
        "* Compare with First Question: Reflect on the system's response to the following question and compare it with the response to the first question above. Note any differences in accuracy, relevance, or clarity of the answers. **(0.5 point)**\n",
        "\n",
        "* Analyze the causes behind these observations. Consider factors such as the nature of the question, the relevance of the chosen chunk, and how the AI model interprets different types of queries. **(0.25 point)**\n",
        "\n",
        "* Propose Changes: Based on your observations, propose potential changes or adjustments that could improve the system's ability to retrieve more accurate or relevant answers **(0.25 point)**. Evaluate Trade-offs: Discuss the trade-offs associated with the changes you propose. **(0.25 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGaVq2vaaB_7"
      },
      "outputs": [],
      "source": [
        "question2 = \"What additional cost does Lean Six Sigma Black Belt Training require?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW30Jf7baDV8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_T7lnzTbOIL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Overall code\n",
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install openai==0.28\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu\n",
        "!pip install nltk\n",
        "!pip install pandas\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "config_ini_location = '/content/drive/MyDrive/Colab Notebooks/config.ini' # Change this to point to the location of your config.ini file.\n",
        "import configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_ini_location)\n",
        "openai_api_key = config['OpenAI']['API_KEY']\n",
        "\n",
        "model_name=\"gpt-3.5-turbo-0613\" # Do Not change this!\n",
        "\n",
        "syllabus_corpus_path = \"/content/drive/MyDrive/Colab Notebooks/IS883_HW4/IS883_HW4_syllabus_corpus\"\n",
        "\n",
        "\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# Path to your PDF directory\n",
        "syllabus_corpus_path = \"/content/drive/MyDrive/Colab Notebooks/IS883_HW4/IS883_HW4_syllabus_corpus\"\n",
        "\n",
        "# Initialize the PDF loader\n",
        "pdf_loader = PyPDFDirectoryLoader(syllabus_corpus_path)\n",
        "\n",
        "# Load documents\n",
        "documents = pdf_loader.load()\n",
        "\n",
        "# Check if any documents are loaded\n",
        "print(f\"Number of documents loaded: {len(documents)}\")\n",
        "\n",
        "# Initialize the text splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 750,\n",
        "    chunk_overlap = 100\n",
        ")\n",
        "\n",
        "chunks = pdf_loader.load_and_split(text_splitter)\n",
        "\n",
        "import faiss\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "import faiss\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "db = FAISS.from_documents(chunks,embeddings)\n",
        "\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "chain = load_qa_chain(OpenAI(openai_api_key=openai_api_key), chain_type=\"stuff\")\n",
        "\n",
        "query = \"Who published the City code?\"\n",
        "docs = db.similarity_search(query, k =2)\n",
        "result = chain.run(input_documents = docs, question = query)\n",
        "\n",
        "print(result)\n",
        "\n",
        "query = \"What is the limitation on campaign spending in city preliminary elections and city elections?\"\n",
        "\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "print(docs[0].page_content)\n",
        "\n",
        "\n",
        "\n",
        "import faiss\n",
        "import openai\n",
        "import numpy as np\n",
        "import configparser\n",
        "\n",
        "# Location of your config.ini file\n",
        "config_ini_location = '/content/drive/MyDrive/Colab Notebooks/config.ini'\n",
        "\n",
        "# Read the API key from the config file\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_ini_location)\n",
        "openai_api_key = config['OpenAI']['API_KEY']\n",
        "\n",
        "# Set the OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Function to get embeddings using OpenAI\n",
        "def get_openai_embedding(text):\n",
        "    response = openai.Embedding.create(input=text, engine=\"text-similarity-babbage-001\")\n",
        "    return np.array(response['data'][0]['embedding'])\n",
        "\n",
        "# Assuming 'chunks' is a list of text chunks from your previous step\n",
        "\n",
        "# Calculate embeddings for each chunk\n",
        "embeddings = [get_openai_embedding(chunk) for chunk in chunks]\n",
        "# embeddings = [get_openai_embedding(chunk.text_content) for chunk in chunks]\n",
        "\n",
        "\n",
        "# Create a FAISS index\n",
        "dimension = len(embeddings[0])  # Dimension of the embeddings\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(np.array(embeddings))\n",
        "\n",
        "# Function to search in the index\n",
        "def search(query):\n",
        "    query_embedding = get_openai_embedding(query)\n",
        "    distances, indices = index.search(np.array([query_embedding]), k=1)  # k is the number of nearest neighbors\n",
        "    return chunks[indices[0][0]]\n",
        "\n",
        "# Example usage\n",
        "query = \"What is Councilor Worrell's first name?\"\n",
        "answer_chunk = search(query)\n",
        "print(answer_chunk)\n",
        "\n",
        "\n",
        "for document in documents:\n",
        "    print(dir(document))\n",
        "    break  # Only print for the first document to avoid too much output\n",
        "\n",
        "\n",
        "question = \"Who is the instructor of Linear Algebra III?\"\n",
        "\n",
        "import os\n",
        "\n",
        "source_info = []\n",
        "\n",
        "for document in documents:\n",
        "    # Extract text from each document\n",
        "    extracted_text = document.page_content\n",
        "\n",
        "    # Extract document name and page number from metadata\n",
        "    document_name = os.path.basename(document.metadata.get('source', 'Unknown Document'))\n",
        "    page_number = document.metadata.get('page', 'Unknown Page')\n",
        "\n",
        "    # Split the extracted text into chunks\n",
        "    chunks = text_splitter.split_text(extracted_text)\n",
        "\n",
        "    # Store source information for each chunk\n",
        "    for chunk in chunks:\n",
        "        source_info.append((document_name, page_number))\n",
        "\n",
        "\n",
        "def similarity_search(query):\n",
        "    query_embedding = get_openai_embedding(query)\n",
        "    distances, indices = index.search(np.array([query_embedding]), k=1)  # k is the number of nearest neighbors\n",
        "    best_match_index = indices[0][0]\n",
        "    return chunks[best_match_index], source_info[best_match_index]\n",
        "\n",
        "# Example usage\n",
        "query = \"What is Councilor Worrell's first name?\"\n",
        "answer_chunk, source = similarity_search(query)\n",
        "print(\"Answer:\", answer_chunk)\n",
        "print(\"Source:\", source)\n",
        "\n",
        "\n",
        "def similarity_search(query):\n",
        "    query_embedding = get_openai_embedding(query)\n",
        "    distances, indices = index.search(np.array([query_embedding]), k=5)  # k=5 for top 5 results\n",
        "\n",
        "    results = []\n",
        "    for i in range(5):  # Iterate over top 5 results\n",
        "        chunk_index = indices[0][i]\n",
        "        similarity_score = distances[0][i]\n",
        "        chunk = chunks[chunk_index]\n",
        "        source = source_info[chunk_index]\n",
        "        results.append((chunk, similarity_score, source))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "query = \"Only using the reference text provided (City of Boston Municipal Code) and only answering the question asked to find an answer: what is the first name of the Councilor with last name \"\"Worrell\"\"?\"\n",
        "top_matches = similarity_search(query)\n",
        "\n",
        "print(\"Top 5 Matches:\")\n",
        "for i, (chunk, score, source) in enumerate(top_matches):\n",
        "    print(f\"Match {i+1}: Score = {score}\")\n",
        "    print(f\"Chunk: {chunk}\")\n",
        "    print(f\"Source: {source}\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "\n",
        "# Function to get answer from OpenAI\n",
        "def get_answer_from_openai(question, context):\n",
        "    openai.api_key = openai_api_key\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci\",\n",
        "        prompt=f\"Question: {question}\\n\\nContext: {context}\\n\\nAnswer:\",\n",
        "        temperature = 0.3,\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Example usage\n",
        "query = \"What is the shape of the city seal\"\n",
        "context = top_matches[0][0]  # Most relevant chunk\n",
        "answer = get_answer_from_openai(query, context)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Context:\", context)\n",
        "print(\"Answer from OpenAI:\", answer)\n",
        "\n",
        "\n",
        "# temperature =\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Create a reference to the language model\n",
        "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=temperature, model_name=model_name)\n",
        "\n",
        "\n",
        "def get_answer_from_openai(question, context):\n",
        "    # Craft a prompt that guides the model to use only the provided context\n",
        "    prompt = f\"Based on the following text, answer the question:\\n\\nText: {context}\\n\\nQuestion: {question}\\n\\nAnswer (using only the above text):\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci\",\n",
        "        prompt=prompt,\n",
        "        temperature = 0.1,\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Example usage\n",
        "query = \"Who is the publisher of the Boston municipal code?\"\n",
        "context = top_matches[0][0]  # Most relevant chunk\n",
        "answer = get_answer_from_openai(query, context)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Context:\", context)\n",
        "print(\"Answer from OpenAI:\", answer)\n",
        "\n",
        "\n",
        "def get_answer_from_openai(question, context):\n",
        "    # Simplified prompt\n",
        "    prompt = f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"davinci\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    return response.choices[0].text.strip()\n",
        "\n",
        "# Example usage\n",
        "query = \"What is Councilor Worrell's first name?\"\n",
        "context = top_matches[0][0]  # Most relevant chunk\n",
        "answer = get_answer_from_openai(query, context)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"Context:\", context)\n",
        "print(\"Answer from OpenAI:\", answer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ixHWWmqSeFLa",
        "outputId": "3d84fc72-ac95-457a-e100-d78eb1cff1c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.346-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.1,>=0.0.10 (from langchain)\n",
            "  Downloading langchain_core-0.0.10-py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.69-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.346 langchain-core-0.0.10 langsmith-0.0.69 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.17.1-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.6/277.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.1\n",
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Number of documents loaded: 63\n",
            " I don't know.\n",
            "SustainableAgriculture W ork(Jackson)Food, Farm ing andDemocracy (Lappé)903/10MIDTERM #2:  first half of periodFast Food in A merica:  second halfFast Food N ation, EricSchlosser03/16Final Exam   1:10 – 4:00 PM\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f5df72888205>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Calculate embeddings for each chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_openai_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;31m# embeddings = [get_openai_embedding(chunk.text_content) for chunk in chunks]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-f5df72888205>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Calculate embeddings for each chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_openai_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;31m# embeddings = [get_openai_embedding(chunk.text_content) for chunk in chunks]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-f5df72888205>\u001b[0m in \u001b[0;36mget_openai_embedding\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# Function to get embeddings using OpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_openai_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-similarity-babbage-001\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/embedding.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# If a user specifies base64, we'll just return the encoded string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     ) -> Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], bool, str]:\n\u001b[0;32m--> 288\u001b[0;31m         result = self.request_raw(\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     ) -> requests.Response:\n\u001b[0;32m--> 581\u001b[0;31m         abs_url, headers, data = self._prepare_request_raw(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupplied_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_prepare_request_raw\u001b[0;34m(self, url, supplied_headers, method, params, files, request_id)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
            "\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type Document is not JSON serializable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BykXgjiQOY5"
      },
      "source": [
        "**Answer:**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}